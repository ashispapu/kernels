{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(posloc, negloc):\n",
    "    lexicon = []\n",
    "    count_of_files = 0\n",
    "    for fl in [posloc, negloc]:\n",
    "        onlyfiles = [f for f in listdir(fl) if isfile(join(fl, f))]\n",
    "        for filename in onlyfiles:\n",
    "            count_of_files += 1\n",
    "            with open(join(fl, filename)) as f:\n",
    "                contents = f.readlines()\n",
    "                for l in contents[:hm_lines]:\n",
    "                    all_words = word_tokenize(l.lower())\n",
    "                    lexicon += list(all_words)\n",
    "    print('number of files read', count_of_files)\n",
    "    lexicon = [lemmatizer.lemmatize(i.lower()) for i in lexicon]\n",
    "    w_counts = Counter(lexicon)\n",
    "    \n",
    "    l2 = []\n",
    "    for w in w_counts:\n",
    "        if 1000 > w_counts[w] > 50:\n",
    "            l2.append(w.lower())\n",
    "            \n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_handling(sample_list: list, lexicon, classification):\n",
    "    featureset = []\n",
    "    for sample in sample_list:\n",
    "        print('Trying to read file', sample)\n",
    "        with open(sample) as f:\n",
    "            contents = f.readlines()\n",
    "            for l in contents[:hm_lines]:\n",
    "                current_words = word_tokenize(l.lower())\n",
    "                current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "                features = np.zeros(len(lexicon))\n",
    "                for word in current_words:\n",
    "                    for word in lexicon:\n",
    "                        index_value = lexicon.index(word.lower())\n",
    "                        features[index_value] += 1\n",
    "                features = list(features)\n",
    "                featureset.append([features, classification])\n",
    "    return featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_sets_and_labels(posloc, negloc, test_size=0.1):\n",
    "    lexicon = create_lexicon(posloc, negloc)\n",
    "    posfiles = [join(posloc, f) for f in listdir(posloc) if isfile(join(posloc, f))]\n",
    "    features = sample_handling(posfiles[:30], lexicon, [1, 0])\n",
    "    print('len of features', len(features))\n",
    "    negfiles = [join(negloc, f) for f in listdir(negloc) if isfile(join(negloc, f))]\n",
    "    features += sample_handling(negfiles[:30], lexicon, [0, 1])\n",
    "    print('len of features', len(features))\n",
    "    random.shuffle(features)\n",
    "    \n",
    "    testing_size = int(test_size*len(features))\n",
    "    features = np.array(features)\n",
    "    train_x = list(features[:, 0][:-testing_size])\n",
    "    train_y = list(features[:, 1][:-testing_size])\n",
    "    \n",
    "    \n",
    "    test_x = list(features[:, 0][-testing_size:])\n",
    "    test_y = list(features[:, 1][-testing_size:])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files read 2000\n",
      "Trying to read file txt_sentoken/pos/cv839_21467.txt\n",
      "Trying to read file txt_sentoken/pos/cv034_29647.txt\n",
      "Trying to read file txt_sentoken/pos/cv908_16009.txt\n",
      "Trying to read file txt_sentoken/pos/cv748_12786.txt\n",
      "Trying to read file txt_sentoken/pos/cv253_10077.txt\n",
      "Trying to read file txt_sentoken/pos/cv147_21193.txt\n",
      "Trying to read file txt_sentoken/pos/cv962_9803.txt\n",
      "Trying to read file txt_sentoken/pos/cv686_13900.txt\n",
      "Trying to read file txt_sentoken/pos/cv410_24266.txt\n",
      "Trying to read file txt_sentoken/pos/cv913_29252.txt\n",
      "Trying to read file txt_sentoken/pos/cv695_21108.txt\n",
      "Trying to read file txt_sentoken/pos/cv601_23453.txt\n",
      "Trying to read file txt_sentoken/pos/cv490_17872.txt\n",
      "Trying to read file txt_sentoken/pos/cv518_13331.txt\n",
      "Trying to read file txt_sentoken/pos/cv157_29372.txt\n",
      "Trying to read file txt_sentoken/pos/cv570_29082.txt\n",
      "Trying to read file txt_sentoken/pos/cv289_6463.txt\n",
      "Trying to read file txt_sentoken/pos/cv098_15435.txt\n",
      "Trying to read file txt_sentoken/pos/cv656_24201.txt\n",
      "Trying to read file txt_sentoken/pos/cv878_15694.txt\n",
      "Trying to read file txt_sentoken/pos/cv172_11131.txt\n",
      "Trying to read file txt_sentoken/pos/cv152_8736.txt\n",
      "Trying to read file txt_sentoken/pos/cv762_13927.txt\n",
      "Trying to read file txt_sentoken/pos/cv028_26746.txt\n",
      "Trying to read file txt_sentoken/pos/cv756_22540.txt\n",
      "Trying to read file txt_sentoken/pos/cv595_25335.txt\n",
      "Trying to read file txt_sentoken/pos/cv701_14252.txt\n",
      "Trying to read file txt_sentoken/pos/cv407_22637.txt\n",
      "Trying to read file txt_sentoken/pos/cv794_15868.txt\n",
      "Trying to read file txt_sentoken/pos/cv000_29590.txt\n",
      "len of features 958\n",
      "Trying to read file txt_sentoken/neg/cv676_22202.txt\n",
      "Trying to read file txt_sentoken/neg/cv839_22807.txt\n",
      "Trying to read file txt_sentoken/neg/cv155_7845.txt\n",
      "Trying to read file txt_sentoken/neg/cv465_23401.txt\n",
      "Trying to read file txt_sentoken/neg/cv398_17047.txt\n",
      "Trying to read file txt_sentoken/neg/cv206_15893.txt\n",
      "Trying to read file txt_sentoken/neg/cv037_19798.txt\n",
      "Trying to read file txt_sentoken/neg/cv279_19452.txt\n",
      "Trying to read file txt_sentoken/neg/cv646_16817.txt\n",
      "Trying to read file txt_sentoken/neg/cv756_23676.txt\n",
      "Trying to read file txt_sentoken/neg/cv823_17055.txt\n",
      "Trying to read file txt_sentoken/neg/cv747_18189.txt\n",
      "Trying to read file txt_sentoken/neg/cv258_5627.txt\n",
      "Trying to read file txt_sentoken/neg/cv948_25870.txt\n",
      "Trying to read file txt_sentoken/neg/cv744_10091.txt\n",
      "Trying to read file txt_sentoken/neg/cv754_7709.txt\n",
      "Trying to read file txt_sentoken/neg/cv838_25886.txt\n",
      "Trying to read file txt_sentoken/neg/cv131_11568.txt\n",
      "Trying to read file txt_sentoken/neg/cv401_13758.txt\n",
      "Trying to read file txt_sentoken/neg/cv523_18285.txt\n",
      "Trying to read file txt_sentoken/neg/cv073_23039.txt\n",
      "Trying to read file txt_sentoken/neg/cv688_7884.txt\n",
      "Trying to read file txt_sentoken/neg/cv664_4264.txt\n",
      "Trying to read file txt_sentoken/neg/cv461_21124.txt\n",
      "Trying to read file txt_sentoken/neg/cv909_9973.txt\n",
      "Trying to read file txt_sentoken/neg/cv939_11247.txt\n",
      "Trying to read file txt_sentoken/neg/cv368_11090.txt\n",
      "Trying to read file txt_sentoken/neg/cv185_28372.txt\n",
      "Trying to read file txt_sentoken/neg/cv749_18960.txt\n",
      "Trying to read file txt_sentoken/neg/cv836_14311.txt\n",
      "len of features 1918\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, test_x, test_y = create_feature_sets_and_labels('txt_sentoken/pos/', 'txt_sentoken/neg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2316"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "\n",
    "x = tf.placeholder('float', [None, len(train_x[0])])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(data):\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))} \n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2 ])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
    "                      'biases': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), \n",
    "                    'biases': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    layer1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    layer2 = tf.add(tf.matmul(layer1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    \n",
    "    layer3 = tf.add(tf.matmul(layer2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    \n",
    "    output = tf.matmul(layer3, output_layer['weights']) + output_layer['biases']\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network(x)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    how_many_epochs = 100\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(how_many_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "                \n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict= {x: batch_x, y: batch_y})\n",
    "            \n",
    "                epoch_loss += c\n",
    "                i = end\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                print('Epoch', epoch, 'completed out of', how_many_epochs, 'loss', epoch_loss)\n",
    "            \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy', accuracy.eval({x: test_x, y: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 100 loss 15944706.52734375\n",
      "Epoch 5 completed out of 100 loss 4950909.814453125\n",
      "Epoch 10 completed out of 100 loss 4302392.155761719\n",
      "Epoch 15 completed out of 100 loss 4318885.634521484\n",
      "Epoch 20 completed out of 100 loss 8332908.4609375\n",
      "Epoch 25 completed out of 100 loss 2356580.990234375\n",
      "Epoch 30 completed out of 100 loss 3251262.5595703125\n",
      "Epoch 35 completed out of 100 loss 2304494.6352539062\n",
      "Epoch 40 completed out of 100 loss 1665516.2199707031\n",
      "Epoch 45 completed out of 100 loss 1684218.154296875\n",
      "Epoch 50 completed out of 100 loss 1462201.861328125\n",
      "Epoch 55 completed out of 100 loss 1403230.7802734375\n",
      "Epoch 60 completed out of 100 loss 1501104.6137695312\n",
      "Epoch 65 completed out of 100 loss 1811004.75390625\n",
      "Epoch 70 completed out of 100 loss 2315435.5290527344\n",
      "Epoch 75 completed out of 100 loss 918950.328125\n",
      "Epoch 80 completed out of 100 loss 2490603.805908203\n",
      "Epoch 85 completed out of 100 loss 3452183.6724243164\n",
      "Epoch 90 completed out of 100 loss 2596698.221923828\n",
      "Epoch 95 completed out of 100 loss 911023.8803710938\n",
      "Accuracy 0.46596858\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
